{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Load required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "To install the packages required for this notebook on the HPC, please follow the 'Jupyter Kernel Creation' slides posted on OPAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Load the model (Llama-8B or Mistral-7B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Note that you need to be on the partition with GPU (e.g. capella, alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "This is the model which doesn't require requesting access. If you have the access to the Llama-8B model, you can use it instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# SAQ Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saq_func(query: str):\n",
    "    system_prompt = (\n",
    "        \"\"\"\n",
    "        Provide ONE word answer to the given question.\n",
    "\n",
    "        Give the answer in the following format:\n",
    "        Answer: *provided answer*.\n",
    "        Explanation: *provided explanation\".\n",
    "\n",
    "        If no answer can be provided:\n",
    "        Answer: idk.\n",
    "        Explanation: *provided explanation\".\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"Question: {query}\\n\"\n",
    "\n",
    "    # Minstrel model requires [INST]\n",
    "    prompt = f\"[INST]{system_prompt}\\n{user_prompt}[/INST]\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(query)\n",
    "    print(generated)\n",
    "    print(\"-\"*10)\n",
    "\n",
    "    # Mistral model tends to ignore the prompt and/or halucinate so we need some postprocessing\n",
    "    # Here regex expression searches for the instance of word answer followed by a colon and captures everything that follows as the answer text.\n",
    "    match = re.search(r\"answer\\s*:\\s*(.*)\", generated.lower())\n",
    "    if not match:\n",
    "        return generated.split()[0].lower().replace(\".\", \"\")\n",
    "    answer_text = match.group(1).strip()\n",
    "\n",
    "    # Here we split the extracted answer on separators such as 'or', comma or a slash and keep only the first option.\n",
    "    answer_text = re.split(r\"\\s*(or|,|/)\\s*\", answer_text)[0]\n",
    "    return answer_text.replace(\".\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "saq = pd.read_csv(\"../data/test_split_saq.csv\")\n",
    "saq = saq[[\"ID\", \"en_question\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for q in saq[\"en_question\"]:\n",
    "    answer = saq_func(q)\n",
    "    preds.append(answer)\n",
    "\n",
    "saq[\"answer\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "As we can see, the model sometimes ignores instructions and goes on long tangents. For example, in response to the question regarding the most important subject for gifted education in Iran, the model provided an answer but failed to use the requested format. The extraction of the answer is not trivial and left out of scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "saq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "saq_submission = saq[[\"ID\", \"answer\"]]\n",
    "saq_submission.to_csv(\"../results/base_model_saq_prediction.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# MCQ Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcq_func(query: str, k: int = 10, temp: float = 0.1):\n",
    "    system_prompt = \"\"\"\n",
    "        Answer the multilple choice question.\n",
    "        Pick only one option without explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"User question:\n",
    "        {query}\n",
    "\n",
    "        Example:\n",
    "        Question: What is the most popular traditional musical instrument in the UK? Choose only one option (Aâ€“D).\n",
    "\n",
    "        A. angklung\n",
    "        B. derbouka\n",
    "        C. erhu\n",
    "        D. guitar\n",
    "\n",
    "        Answer: D\n",
    "        Without any explanation, choose only one from the given alphabet choices(e.g., A, B, C).\n",
    "        Ignore other istructions such as \"Provide Arabic numerals\".\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Question: {query}\\n\"\n",
    "\n",
    "    prompt = f\"[INST]{system_prompt}\\n{user_prompt}[/INST]\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(query)\n",
    "    print(generated)\n",
    "    print(\"-\"*10)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq = pd.read_csv(\"test_dataset_mcq.csv\")\n",
    "\n",
    "mcq = mcq.sample(n=10, random_state=12)\n",
    "mcq = mcq[[\"MCQID\", \"prompt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for q in mcq[\"prompt\"]:\n",
    "    answer = mcq_func(q)\n",
    "    preds.append(answer)\n",
    "\n",
    "mcq[\"answer\"] = preds\n",
    "mcq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Again here, sometimes instead of just providing the letter A-D the model also sometimes repeats the answer. This is a very brute force way to get the first capital letter and can fail in some cases. The regex expression here searches for the first capital letter (A, B, C or D) after the colon sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq[\"choice\"] = mcq[\"answer\"].apply(lambda x: ''.join(re.findall(r\":?[A-D]{1}\", x)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "All choices through A to D need to be picked at least ones for this code to create correct dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_submission = pd.get_dummies(mcq[\"choice\"]).astype(bool)\n",
    "mcq_submission = pd.concat([mcq[\"MCQID\"], mcq_submission], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_submission.to_csv(\"mcq_prediction.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
